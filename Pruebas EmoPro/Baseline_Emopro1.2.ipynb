{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline de algoritmos de xtracción de palabras clave"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos básicos: TextRank y Fractalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook está hecha para correr TextRank y Grado de Fractalidad sobre los textos con etiqueta 0 o etiqueta 5 comparando resultados con la base de datos del EmoPro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importes para librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from math import sqrt\n",
    "import string\n",
    "# import csv\n",
    "import operator\n",
    "# import random\n",
    "import pandas as pd\n",
    "#librerias necesarias para text rank\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "#import nlp\n",
    "\n",
    "#Listado de STOPWORDS dependiendo del lenguaje\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la clase TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 100 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "\n",
    "\n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        keysw={}\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            keysw[key] =value\n",
    "            if i > number:\n",
    "                break\n",
    "        return keysw\n",
    "\n",
    "\n",
    "    def analyze(self, text,\n",
    "                candidate_pos=['NOUN', 'PROPN'],\n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la función Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solamente se calcula el grado de fractalidad de las palabras que tengan mas de uno de frecuencia\n",
    "def fractalidad(palabras,vocabulario,frec,dist):\n",
    "    N=len(palabras)                                     #El número de tokens de todo el texto\n",
    "    gf={}\n",
    "    cajas_index=set()\n",
    "    voc=[]                                             #la variable voc contendra cada sintagma con frecuencia mayor que 1, por que las otras palabras tendrán 0 de grado de fractaldiad\n",
    "    for p in vocabulario:                              #Esto se puede hacer fuera del algoritmo, pero se incluye para evitar ese calculo innecesario\n",
    "        if(p not in voc):\n",
    "            if(frec[p]>1):\n",
    "                if(p not in STOP_WORDS):\n",
    "                    if(len(p)>1):\n",
    "                        voc.append(p)\n",
    "    # print(\"Text size: \",N)\n",
    "    # print(\"Vocabulary: \",len(voc))\n",
    "    for p in voc:\n",
    "        rcajas=dist[p]\n",
    "        M=frec[p]\n",
    "        dfw=0.0\n",
    "        nsh=0.0\n",
    "        for s in range(1,N+1):\n",
    "            noc=0\n",
    "            for e in rcajas:\n",
    "                cajas_index.add(ceil(int(e)/s))\n",
    "            noc=len(cajas_index)\n",
    "            cajas_index.clear()\n",
    "            ns=N/s\n",
    "            if(M<=ns):\n",
    "                nsh=M\n",
    "            else:\n",
    "                nsh=M/(1+(M-1)/(N-1)*(s-1))\n",
    "            dfw=dfw+fabs(log(nsh/noc))\n",
    "        gf[p]=dfw\n",
    "    return gf    #regresamos un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribucion(palabras,vocabulario):\n",
    "    N=len(palabras)\n",
    "    ncajas=[]\n",
    "    cajas={}\n",
    "    frecuencias={}\n",
    "    for p in vocabulario:\n",
    "        ncajas.clear()\n",
    "        i=0\n",
    "        M=palabras.count(p)\n",
    "        while(i<N):\n",
    "            if(p == palabras[i]):\n",
    "                ncajas.append(i+1)\n",
    "            i=i+1\n",
    "        frecuencias[p]=M\n",
    "        cajas[p]=ncajas[:]\n",
    "    return frecuencias,cajas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de archivo para generación de vocabulario\n",
    "def cargar_datos(filename):\n",
    "    textopos = []\n",
    "    textoneg =[]\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        texto = pd.read_csv(f)\n",
    "        for _, r in texto.iterrows():\n",
    "            if r['ds0'] == 1:\n",
    "                textoneg.append(r['Texto'])\n",
    "            elif r['ds5']==1:\n",
    "                textopos.append(r['Texto'])\n",
    "        for row in textopos:\n",
    "            #Pasar a minusculas\n",
    "            row = ' '.join(row)\n",
    "            row = row.lower()\n",
    "    #Eliminar puntuación\n",
    "        for row in textoneg:\n",
    "            row = ' '.join(row)\n",
    "            row = row.lower()\n",
    "    textopos =' '.join(textopos)\n",
    "    textoneg=' '.join(textoneg)\n",
    "    textopos = textopos.translate(str.maketrans('', '', string.punctuation))\n",
    "    textoneg = textoneg.translate(str.maketrans('', '', string.punctuation))\n",
    "    textopos = textopos.translate(str.maketrans('', '', '¿?¡!.:,;—“”0123456789’'))\n",
    "    textoneg = textoneg.translate(str.maketrans('', '', '¿?¡!.:,;—“”0123456789’'))\n",
    "    palabraspos = textopos.split()\n",
    "    palabrasneg = textoneg.split()\n",
    "    textop=\"\"\n",
    "    texton=\"\"\n",
    "    #rearmamos el texto debido a que existen carácteres especiales\n",
    "    for w in palabraspos:\n",
    "        textop=textop+w+' '\n",
    "    for w in palabrasneg:\n",
    "        texton=texton+w+' '\n",
    "    return textop, texton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_documento(filename='NULL',texto=''):\n",
    "    if filename != 'NULL':\n",
    "        textop, texton=cargar_datos(filename)\n",
    "    textop = textop.lower()\n",
    "    texton = texton.lower()\n",
    "    #obtenemos el vocabulario\n",
    "    tokensp=textop.split()\n",
    "    vocabulariop=[]\n",
    "    vocabularion=[]\n",
    "    tokensn = texton.split()\n",
    "    for t in tokensp:\n",
    "        if(t not in vocabulariop):\n",
    "            vocabulariop.append(t)\n",
    "    for t in tokensn:\n",
    "        if(t not in vocabularion):\n",
    "            vocabularion.append(t)\n",
    "    #variables de procesamiento\n",
    "    distp={}\n",
    "    distn={}\n",
    "    frecn={}\n",
    "    frecp={}\n",
    "    frecp,distp=distribucion(tokensp,vocabulariop)\n",
    "    frecn, distn = distribucion(tokensn, vocabularion)\n",
    "    return frecp,distp,tokensp,vocabulariop,textop,frecn,distn,tokensn,vocabularion,texton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de documento para el corpus de suicidio\n",
    "frecp,distp,tokensp,vocabulariop,textop,frecn,distn,tokensn,vocabularion,texton = lee_documento('/home/matias/Documentos/MGP/Proyecto_tesis/Dataset/SMS_DATA_ORIGINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuencias: 8070\n",
      "Distribuciones:  8070\n",
      "Tokens:  101671\n",
      "Vocabulario:  8070\n",
      "Texto:  544467\n"
     ]
    }
   ],
   "source": [
    "print('Frecuencias:', len(frecp))\n",
    "print(\"Distribuciones: \", len(distp))\n",
    "print(\"Tokens: \", len(tokensp))\n",
    "print('Vocabulario: ', len(vocabulariop))\n",
    "print('Texto: ', len(textop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuencias: 3349\n",
      "Distribuciones:  3349\n",
      "Tokens:  25052\n",
      "Vocabulario:  3349\n",
      "Texto:  130002\n"
     ]
    }
   ],
   "source": [
    "print('Frecuencias:', len(frecn))\n",
    "print(\"Distribuciones: \", len(distn))\n",
    "print(\"Tokens: \", len(tokensn))\n",
    "print('Vocabulario: ', len(vocabularion))\n",
    "print('Texto: ', len(texton))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejecución de algoritmo Grado de Fractalidad\n",
    "def grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw=False,regresa_df=True,top_n=np.inf,escribe_arch=False):\n",
    "    frac_x=fractalidad(tokens,vocabulario,frec,dist)\n",
    "    sorted_x = sorted(frac_x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # print('Time GF: '+str(elapsed_time))\n",
    "\n",
    "    #Imprimir y guardar resultados de GF\n",
    "    if regresa_df:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            df = [dato[0] for dato in df]\n",
    "            by_MC=pd.DataFrame(df, columns=['word'])\n",
    "        else:\n",
    "            by_MC=pd.DataFrame(df, columns=['word','frecuency','Degree_of_fractality','Combined_measure'])\n",
    "        if escribe_arch:\n",
    "            by_MC.to_csv('GF.csv')\n",
    "    else:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            by_MC = [dato[0] for dato in df]\n",
    "        else:\n",
    "            by_MC = df\n",
    "        if escribe_arch:\n",
    "            print('\\nNo se tiene implementada la escritura de archivo cuando regresa_df==False\\n')\n",
    "    return by_MC\n",
    "\n",
    "def use_gf(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tokens=texto.split()\n",
    "    vocabulario=[]\n",
    "    for t in tokens:\n",
    "        if(t not in vocabulario):\n",
    "            vocabulario.append(t)\n",
    "    #variables de procesamiento\n",
    "    dist={}\n",
    "    frec={}\n",
    "    frec,dist=distribucion(tokens,vocabulario)\n",
    "    df = grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw,regresa_df,top_n,escribe_arch)\n",
    "    return df\n",
    "\n",
    "fractPositivas = use_gf(textop,regresa_kw=True,regresa_df=True,top_n=1286)\n",
    "fractNegativas = use_gf(texton,regresa_kw=True,regresa_df=True,top_n=1286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caca</td>\n",
       "      <td>23</td>\n",
       "      <td>85221.880907</td>\n",
       "      <td>116049.007469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok</td>\n",
       "      <td>12</td>\n",
       "      <td>78526.466331</td>\n",
       "      <td>84744.289783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capsulas</td>\n",
       "      <td>11</td>\n",
       "      <td>78681.018360</td>\n",
       "      <td>81937.836981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>observaciones</td>\n",
       "      <td>19</td>\n",
       "      <td>63249.000502</td>\n",
       "      <td>80879.887149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calzoncillo</td>\n",
       "      <td>11</td>\n",
       "      <td>77619.128001</td>\n",
       "      <td>80831.992129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>sentiría</td>\n",
       "      <td>2</td>\n",
       "      <td>31522.006956</td>\n",
       "      <td>9489.069617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>impulsivo</td>\n",
       "      <td>2</td>\n",
       "      <td>31511.197263</td>\n",
       "      <td>9485.815576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>frecuentes</td>\n",
       "      <td>2</td>\n",
       "      <td>31441.375211</td>\n",
       "      <td>9464.797043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>actuó</td>\n",
       "      <td>2</td>\n",
       "      <td>31424.739678</td>\n",
       "      <td>9459.789249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>brusco</td>\n",
       "      <td>2</td>\n",
       "      <td>31419.458523</td>\n",
       "      <td>9458.199463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1286 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0              caca         23          85221.880907     116049.007469\n",
       "1                ok         12          78526.466331      84744.289783\n",
       "2          capsulas         11          78681.018360      81937.836981\n",
       "3     observaciones         19          63249.000502      80879.887149\n",
       "4       calzoncillo         11          77619.128001      80831.992129\n",
       "...             ...        ...                   ...               ...\n",
       "1281       sentiría          2          31522.006956       9489.069617\n",
       "1282      impulsivo          2          31511.197263       9485.815576\n",
       "1283     frecuentes          2          31441.375211       9464.797043\n",
       "1284          actuó          2          31424.739678       9459.789249\n",
       "1285         brusco          2          31419.458523       9458.199463\n",
       "\n",
       "[1286 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractPositivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ángel</td>\n",
       "      <td>10</td>\n",
       "      <td>18176.971038</td>\n",
       "      <td>18176.971038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tío</td>\n",
       "      <td>18</td>\n",
       "      <td>12706.403384</td>\n",
       "      <td>15949.998807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siento</td>\n",
       "      <td>118</td>\n",
       "      <td>6956.870176</td>\n",
       "      <td>14413.814144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vómitos</td>\n",
       "      <td>7</td>\n",
       "      <td>16922.938652</td>\n",
       "      <td>14301.542286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vida</td>\n",
       "      <td>82</td>\n",
       "      <td>7257.344789</td>\n",
       "      <td>13889.206989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>edad</td>\n",
       "      <td>2</td>\n",
       "      <td>1798.162775</td>\n",
       "      <td>541.300932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>grosero</td>\n",
       "      <td>2</td>\n",
       "      <td>1788.353395</td>\n",
       "      <td>538.348015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>seria</td>\n",
       "      <td>2</td>\n",
       "      <td>1772.230261</td>\n",
       "      <td>533.494468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>ayuden</td>\n",
       "      <td>2</td>\n",
       "      <td>1764.830483</td>\n",
       "      <td>531.266913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>agua</td>\n",
       "      <td>2</td>\n",
       "      <td>1747.438569</td>\n",
       "      <td>526.031425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0       ángel         10          18176.971038      18176.971038\n",
       "1         tío         18          12706.403384      15949.998807\n",
       "2      siento        118           6956.870176      14413.814144\n",
       "3     vómitos          7          16922.938652      14301.542286\n",
       "4        vida         82           7257.344789      13889.206989\n",
       "...       ...        ...                   ...               ...\n",
       "1115     edad          2           1798.162775        541.300932\n",
       "1116  grosero          2           1788.353395        538.348015\n",
       "1117    seria          2           1772.230261        533.494468\n",
       "1118   ayuden          2           1764.830483        531.266913\n",
       "1119     agua          2           1747.438569        526.031425\n",
       "\n",
       "[1120 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractNegativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guarda los resultados del grado de fractalidad en un CSV. Siempre hay que cambiar la versión y comentar para no sobreescribir\n",
    "fractPositivas.to_csv('/home/matias/Documentos/MGP/Proyecto_tesis/output/fractalidad_etiqueta5_EmoPro_prueba1.csv')\n",
    "fractNegativas.to_csv('/home/matias/Documentos/MGP/Proyecto_tesis/output/fractalidad_etiqueta0_EmoPro_prueba1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio del máximo de tamaño de texto para que quepa el corpus (1.3 millones)\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejecución de algoritmo de TextRank\n",
    "# start_time = time()\n",
    "def use_TextRank(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(texto, candidate_pos = ['NOUN','PROPN'], window_size=4, lower=False)\n",
    "    kwTR=tr4w.get_keywords(100)\n",
    "\n",
    "    #Guardar resultados de TextRank\n",
    "    if regresa_df:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())[:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "    else:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())[:top_n]\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())\n",
    "        # elapsed_time = time() - start_time\n",
    "        # print('Time TextRank: '+str(elapsed_time))\n",
    "        if escribe_arch:\n",
    "            dftr.to_csv('TextRank.csv')\n",
    "    return dftr\n",
    "\n",
    "trpos = use_TextRank(textop,top_n=1286)\n",
    "trneg = use_TextRank(texton,top_n=1286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trabajo',\n",
       " 'cosas',\n",
       " 'casa',\n",
       " 'semana',\n",
       " 'tiempo',\n",
       " 'vida',\n",
       " 'ansiedad',\n",
       " 'horas',\n",
       " 'noche',\n",
       " 'familia',\n",
       " 'mañana',\n",
       " 'ganas',\n",
       " 'problemas',\n",
       " 'madre',\n",
       " 'cabeza',\n",
       " 'situación',\n",
       " 'sueño',\n",
       " 'problema',\n",
       " 'momento',\n",
       " 'tratamiento',\n",
       " 'miedo',\n",
       " 'medicación',\n",
       " 'sensación',\n",
       " 'ánimo',\n",
       " 'dolor',\n",
       " 'hora',\n",
       " 'gente',\n",
       " 'general',\n",
       " 'años',\n",
       " 'marido',\n",
       " 'mujer',\n",
       " 'pareja',\n",
       " 'persona',\n",
       " 'gracias',\n",
       " 'cosa',\n",
       " 'padre',\n",
       " 'momentos',\n",
       " 'hija',\n",
       " 'tema',\n",
       " 'consulta',\n",
       " 'cuestionario',\n",
       " 'doctora',\n",
       " 'forma',\n",
       " 'cita',\n",
       " 'mes',\n",
       " 'vacaciones',\n",
       " 'cambio',\n",
       " 'comida',\n",
       " 'médico',\n",
       " 'intento',\n",
       " 'tensión',\n",
       " 'hijo',\n",
       " 'hermana',\n",
       " 'amigos',\n",
       " 'semanas',\n",
       " 'nervios',\n",
       " 'pastillas',\n",
       " 'hijos',\n",
       " 'niño',\n",
       " 'pensamientos',\n",
       " 'falta',\n",
       " 'baja',\n",
       " 'sábado',\n",
       " 'relación',\n",
       " 'lunes',\n",
       " 'rato',\n",
       " 'amiga',\n",
       " 'cambios',\n",
       " 'dolores',\n",
       " 'hospital',\n",
       " 'año',\n",
       " 'cansancio',\n",
       " 'sigo',\n",
       " 'estrés',\n",
       " 'viernes',\n",
       " 'cuerpo',\n",
       " 'meses',\n",
       " 'madrid',\n",
       " 'discusión',\n",
       " 'enfermedad',\n",
       " 'domingo',\n",
       " 'salud',\n",
       " 'psiquiatra',\n",
       " 'personas',\n",
       " 'mañanas',\n",
       " 'gusto',\n",
       " 'preocupación',\n",
       " 'test',\n",
       " 'ayuda',\n",
       " 'fuerzas',\n",
       " 'compañeros',\n",
       " 'viaje',\n",
       " 'resto',\n",
       " 'mente',\n",
       " 'circunstancias',\n",
       " 'empresa',\n",
       " 'razón',\n",
       " 'actividad',\n",
       " 'miligramo',\n",
       " 'coche',\n",
       " 'hermano',\n",
       " 'confianza']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vida',\n",
       " 'ansiedad',\n",
       " 'ganas',\n",
       " 'cabeza',\n",
       " 'dolor',\n",
       " 'casa',\n",
       " 'madre',\n",
       " 'cosas',\n",
       " 'noche',\n",
       " 'tiempo',\n",
       " 'mierda',\n",
       " 'horas',\n",
       " 'miedo',\n",
       " 'problemas',\n",
       " 'mundo',\n",
       " 'cosa',\n",
       " 'medicación',\n",
       " 'semana',\n",
       " 'cama',\n",
       " 'trabajo',\n",
       " 'persona',\n",
       " 'mañana',\n",
       " 'comida',\n",
       " 'dolores',\n",
       " 'años',\n",
       " 'familia',\n",
       " 'daño',\n",
       " 'culpa',\n",
       " 'situación',\n",
       " 'pena',\n",
       " 'hospital',\n",
       " 'fuerzas',\n",
       " 'hija',\n",
       " 'año',\n",
       " 'gente',\n",
       " 'momento',\n",
       " 'padre',\n",
       " 'hora',\n",
       " 'cuerpo',\n",
       " 'nervios',\n",
       " 'voces',\n",
       " 'personas',\n",
       " 'problema',\n",
       " 'cita',\n",
       " 'gracias',\n",
       " 'sueño',\n",
       " 'pesadillas',\n",
       " 'médico',\n",
       " 'tío',\n",
       " 'cambio',\n",
       " 'sigo',\n",
       " 'atracones',\n",
       " 'tensión',\n",
       " 'mente',\n",
       " 'hijos',\n",
       " 'discusiones',\n",
       " 'pastillas',\n",
       " 'caso',\n",
       " 'sensación',\n",
       " 'asco',\n",
       " 'hermano',\n",
       " 'amiga',\n",
       " 'chocolate',\n",
       " 'doctora',\n",
       " 'pensamientos',\n",
       " 'consulta',\n",
       " 'decisión',\n",
       " 'pruebas',\n",
       " 'resto',\n",
       " 'calle',\n",
       " 'discusión',\n",
       " 'maría',\n",
       " 'baja',\n",
       " 'sentido',\n",
       " 'ángel',\n",
       " 'jueves',\n",
       " 'mujer',\n",
       " 'pareja',\n",
       " 'domingo',\n",
       " 'noches',\n",
       " 'amigos',\n",
       " 'enfermedad',\n",
       " 'lorazepam',\n",
       " 'falta',\n",
       " 'coche',\n",
       " 'tipo',\n",
       " 'ayuda',\n",
       " 'sábado',\n",
       " 'peso',\n",
       " 'seguridad',\n",
       " 'pecho',\n",
       " 'puta',\n",
       " 'piernas',\n",
       " 'alta',\n",
       " 'moral',\n",
       " 'meses',\n",
       " 'hijo',\n",
       " 'trato',\n",
       " 'duermo',\n",
       " 'depresión',\n",
       " 'atracón',\n",
       " 'voz']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guarda los resultados del TextRank en un txt. Siempre hay que cambiar la versión y comentar para no sobreescribir\n",
    "with open('/home/matias/Documentos/MGP/Proyecto_tesis/output/TextRank_resultados_etiqueta5_EmoPro_prueba1.txt','w') as output:\n",
    "    output.write(str(trpos))\n",
    "with open('//home/matias/Documentos/MGP/Proyecto_tesis/output/TextRank_resultado_setiqueta0_EmoPro_prueba1.txt','w') as output:\n",
    "    output.write(str(trneg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de los resultados de fractalidad y TextRank con el conjunto de Palabras Prototípicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/matias/Documentos/MGP/Proyecto_tesis/Palabras_proto/EmoPro-Dataset.csv', encoding=\"utf-8-sig\") as f:\n",
    "        palabrasProto = pd.read_csv(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Few_Raters</th>\n",
       "      <th>Prototypicality_Mean</th>\n",
       "      <th>Prototypicality_SD</th>\n",
       "      <th>Prototypicality_NRaters</th>\n",
       "      <th>Prototypicality_%Raters</th>\n",
       "      <th>Valence_Mean</th>\n",
       "      <th>Valence_SD</th>\n",
       "      <th>Valence_NRaters</th>\n",
       "      <th>Valence_%Raters</th>\n",
       "      <th>...</th>\n",
       "      <th>Concreteness_%Raters</th>\n",
       "      <th>Concreteness_Source</th>\n",
       "      <th>Zipf_EsPal</th>\n",
       "      <th>POS</th>\n",
       "      <th>Emotionality</th>\n",
       "      <th>Family</th>\n",
       "      <th>Very_related_emotions</th>\n",
       "      <th>Pure_word</th>\n",
       "      <th>Dominant_emotion</th>\n",
       "      <th>Emotional_exclusivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandonado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.27</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.36</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>3.07</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandonarse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>2.98</td>\n",
       "      <td>VERB</td>\n",
       "      <td>3.11</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandono</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.50</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.22</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.31</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.70</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abatido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.75</td>\n",
       "      <td>26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.59</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.23</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abatimiento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.20</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>vital</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.38</td>\n",
       "      <td>22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.35</td>\n",
       "      <td>1.53</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.50</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>1.35</td>\n",
       "      <td>vital</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>vitalidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.10</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Duchon2013</td>\n",
       "      <td>3.74</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3.20</td>\n",
       "      <td>vital</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>vitalizar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>2.03</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1.80</td>\n",
       "      <td>vital</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>vulnerabilidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.45</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.49</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.70</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>vulnerable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.24</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.70</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.20</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1286 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word Few_Raters  Prototypicality_Mean  Prototypicality_SD  \\\n",
       "0         abandonado        NaN                  2.35                1.27   \n",
       "1        abandonarse        NaN                  2.00                1.10   \n",
       "2           abandono        NaN                  2.38                1.50   \n",
       "3            abatido        NaN                  3.37                1.42   \n",
       "4        abatimiento        NaN                  3.00                1.37   \n",
       "...              ...        ...                   ...                 ...   \n",
       "1281           vital        NaN                  2.09                1.38   \n",
       "1282       vitalidad        NaN                  2.05                1.10   \n",
       "1283       vitalizar        NaN                  1.89                1.10   \n",
       "1284  vulnerabilidad        NaN                  2.29                1.45   \n",
       "1285      vulnerable        NaN                  2.45                1.47   \n",
       "\n",
       "      Prototypicality_NRaters  Prototypicality_%Raters  Valence_Mean  \\\n",
       "0                          20                     1.00          1.93   \n",
       "1                          21                     1.00          1.89   \n",
       "2                          21                     1.00          2.30   \n",
       "3                          20                     0.95          2.77   \n",
       "4                          20                     0.90          2.80   \n",
       "...                       ...                      ...           ...   \n",
       "1281                       22                     1.00          6.35   \n",
       "1282                       20                     1.00          8.20   \n",
       "1283                       20                     0.95          6.80   \n",
       "1284                       21                     1.00          2.30   \n",
       "1285                       20                     1.00          2.80   \n",
       "\n",
       "      Valence_SD  Valence_NRaters  Valence_%Raters  ... Concreteness_%Raters  \\\n",
       "0           1.12               20             1.00  ...                  1.0   \n",
       "1           0.88               20             0.95  ...                  1.0   \n",
       "2           1.22               20             1.00  ...                  1.0   \n",
       "3           1.75               26             1.00  ...                  1.0   \n",
       "4           1.32               20             1.00  ...                  1.0   \n",
       "...          ...              ...              ...  ...                  ...   \n",
       "1281        1.53               20             1.00  ...                  1.0   \n",
       "1282        0.95               20             1.00  ...                  NaN   \n",
       "1283        1.20               20             1.00  ...                  1.0   \n",
       "1284        1.08               20             1.00  ...                  1.0   \n",
       "1285        1.24               20             1.00  ...                  1.0   \n",
       "\n",
       "      Concreteness_Source  Zipf_EsPal        POS  Emotionality      Family  \\\n",
       "0                     NEW        4.36  ADJECTIVE          3.07   abandonar   \n",
       "1                     NEW        2.98       VERB          3.11   abandonar   \n",
       "2                     NEW        4.31       NOUN          2.70   abandonar   \n",
       "3                     NEW        3.59  ADJECTIVE          2.23      abatir   \n",
       "4                     NEW        3.28       NOUN          2.20      abatir   \n",
       "...                   ...         ...        ...           ...         ...   \n",
       "1281                  NEW        4.50  ADJECTIVE          1.35       vital   \n",
       "1282           Duchon2013        3.74       NOUN          3.20       vital   \n",
       "1283                  NEW        2.03       VERB          1.80       vital   \n",
       "1284                  NEW        3.49       NOUN          2.70  vulnerable   \n",
       "1285                  NEW        3.70  ADJECTIVE          2.20  vulnerable   \n",
       "\n",
       "      Very_related_emotions  Pure_word  Dominant_emotion  \\\n",
       "0                       2.0        0.0           sadness   \n",
       "1                       2.0        0.0           sadness   \n",
       "2                       3.0        0.0           sadness   \n",
       "3                       1.0        1.0           sadness   \n",
       "4                       1.0        1.0           sadness   \n",
       "...                     ...        ...               ...   \n",
       "1281                    0.0        0.0         happiness   \n",
       "1282                    1.0        1.0         happiness   \n",
       "1283                    1.0        1.0         happiness   \n",
       "1284                    1.0        1.0           sadness   \n",
       "1285                    1.0        1.0           sadness   \n",
       "\n",
       "      Emotional_exclusivity  \n",
       "0                     0.396  \n",
       "1                     0.326  \n",
       "2                     0.350  \n",
       "3                     0.379  \n",
       "4                     0.340  \n",
       "...                     ...  \n",
       "1281                  0.706  \n",
       "1282                  0.901  \n",
       "1283                  0.741  \n",
       "1284                  0.288  \n",
       "1285                  0.274  \n",
       "\n",
       "[1286 rows x 59 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabrasProto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para crear la medida de Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medida Jaccard que mide la proporción de elementos de un conjunto en otro\n",
    "def jaccard(s1: set, s2: set):\n",
    "    return len(s1 & s2) / len(s1 | s2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coinsidencia(conjuntoA, conjuntoB):\n",
    "    matches = []\n",
    "    for palabra in conjuntoA:\n",
    "        if palabra in conjuntoB:\n",
    "            matches.append(palabra)\n",
    "    conteo = len(matches)\n",
    "    return matches, conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard de fractalidad positivas:  0.0362610797743755\n",
      "Jaccard de TextRank positivas:  0.00725689404934688\n",
      "Jaccard de fractalidad negativas:  0.04155844155844156\n",
      "Jaccard de TextRank negativas:  0.00725689404934688\n"
     ]
    }
   ],
   "source": [
    "jaccFractalidadPos = jaccard(set(palabrasProto['Word']), set(fractPositivas['word']))\n",
    "jaccTRPos = jaccard(set(palabrasProto['Word']), set(trpos))\n",
    "jaccFractalidadNeg = jaccard(set(palabrasProto['Word']), set(fractNegativas['word']))\n",
    "jaccTRNeg = jaccard(set(palabrasProto['Word']), set(trneg))\n",
    "# _, conteosTR = jaccard(texto, dftr)\n",
    "# _, conteosFract= jaccard(texto, list(df1['word']))\n",
    "# _, conteosProto= jaccard(texto, list(palabrasProto['Palabra']))\n",
    "\n",
    "print('Jaccard de fractalidad positivas: ', jaccFractalidadPos)\n",
    "\n",
    "print('Jaccard de TextRank positivas: ', jaccTRPos)\n",
    "\n",
    "\n",
    "print('Jaccard de fractalidad negativas: ', jaccFractalidadNeg)\n",
    "\n",
    "print('Jaccard de TextRank negativas: ', jaccTRNeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinTRPos, conteoTRPos= coinsidencia(list(palabrasProto['Word']), trpos)\n",
    "coinTRNeg, conteoTRNeg = coinsidencia(list(palabrasProto['Word']), trneg)\n",
    "coinFracPos, conteoFracPos = coinsidencia(list(palabrasProto['Word']), list(fractPositivas['word']))\n",
    "coinFracNeg, conteoFracNeg = coinsidencia(list(palabrasProto['Word']), list(fractNegativas['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo TR positivas: 10\n",
      "Conteo TR negativas: 10\n",
      "Conteo Fractalidad positivas: 90\n",
      "Conteo Fractalidad negativas: 96\n"
     ]
    }
   ],
   "source": [
    "print('Conteo TR positivas:', conteoTRPos)\n",
    "print('Conteo TR negativas:', conteoTRNeg)\n",
    "print('Conteo Fractalidad positivas:', conteoFracPos)\n",
    "print('Conteo Fractalidad negativas:', conteoFracNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altamente prototípicas\n",
    "\n",
    "Esta sección aprovecha los resultados de los algoritmos para revisar la coinsidencia entre los resultados de éstos, y de las palabras altamente prototípicas. \n",
    "\n",
    "El procedimiento es el siguiente:\n",
    "- Selección de las palabras altamente prototípicas del EmoPro (Media de prototipicalidad >= 3.0)\n",
    "- Ajuste de palabras de etiqueta 0 de fractalidad para que el conjunto sea del mismo tamaño que el de AP\n",
    "- Ajuste de palabras de etiqueta 5 de fractalidad para que el conjunto sea del mismo tamaño que el de AP\n",
    "- Comparación de coinsidencias y medida Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Few_Raters</th>\n",
       "      <th>Prototypicality_Mean</th>\n",
       "      <th>Prototypicality_SD</th>\n",
       "      <th>Prototypicality_NRaters</th>\n",
       "      <th>Prototypicality_%Raters</th>\n",
       "      <th>Valence_Mean</th>\n",
       "      <th>Valence_SD</th>\n",
       "      <th>Valence_NRaters</th>\n",
       "      <th>Valence_%Raters</th>\n",
       "      <th>...</th>\n",
       "      <th>Concreteness_%Raters</th>\n",
       "      <th>Concreteness_Source</th>\n",
       "      <th>Zipf_EsPal</th>\n",
       "      <th>POS</th>\n",
       "      <th>Emotionality</th>\n",
       "      <th>Family</th>\n",
       "      <th>Very_related_emotions</th>\n",
       "      <th>Pure_word</th>\n",
       "      <th>Dominant_emotion</th>\n",
       "      <th>Emotional_exclusivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandonado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.27</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.36</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>3.07</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandonarse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>2.98</td>\n",
       "      <td>VERB</td>\n",
       "      <td>3.11</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandono</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.50</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.22</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.31</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.70</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abatido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.75</td>\n",
       "      <td>26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.59</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.23</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abatimiento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.20</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>vital</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.38</td>\n",
       "      <td>22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.35</td>\n",
       "      <td>1.53</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.50</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>1.35</td>\n",
       "      <td>vital</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>vitalidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.10</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Duchon2013</td>\n",
       "      <td>3.74</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3.20</td>\n",
       "      <td>vital</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>vitalizar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>2.03</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1.80</td>\n",
       "      <td>vital</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>vulnerabilidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.45</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.49</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.70</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>vulnerable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.24</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.70</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.20</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1286 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word Few_Raters  Prototypicality_Mean  Prototypicality_SD  \\\n",
       "0         abandonado        NaN                  2.35                1.27   \n",
       "1        abandonarse        NaN                  2.00                1.10   \n",
       "2           abandono        NaN                  2.38                1.50   \n",
       "3            abatido        NaN                  3.37                1.42   \n",
       "4        abatimiento        NaN                  3.00                1.37   \n",
       "...              ...        ...                   ...                 ...   \n",
       "1281           vital        NaN                  2.09                1.38   \n",
       "1282       vitalidad        NaN                  2.05                1.10   \n",
       "1283       vitalizar        NaN                  1.89                1.10   \n",
       "1284  vulnerabilidad        NaN                  2.29                1.45   \n",
       "1285      vulnerable        NaN                  2.45                1.47   \n",
       "\n",
       "      Prototypicality_NRaters  Prototypicality_%Raters  Valence_Mean  \\\n",
       "0                          20                     1.00          1.93   \n",
       "1                          21                     1.00          1.89   \n",
       "2                          21                     1.00          2.30   \n",
       "3                          20                     0.95          2.77   \n",
       "4                          20                     0.90          2.80   \n",
       "...                       ...                      ...           ...   \n",
       "1281                       22                     1.00          6.35   \n",
       "1282                       20                     1.00          8.20   \n",
       "1283                       20                     0.95          6.80   \n",
       "1284                       21                     1.00          2.30   \n",
       "1285                       20                     1.00          2.80   \n",
       "\n",
       "      Valence_SD  Valence_NRaters  Valence_%Raters  ... Concreteness_%Raters  \\\n",
       "0           1.12               20             1.00  ...                  1.0   \n",
       "1           0.88               20             0.95  ...                  1.0   \n",
       "2           1.22               20             1.00  ...                  1.0   \n",
       "3           1.75               26             1.00  ...                  1.0   \n",
       "4           1.32               20             1.00  ...                  1.0   \n",
       "...          ...              ...              ...  ...                  ...   \n",
       "1281        1.53               20             1.00  ...                  1.0   \n",
       "1282        0.95               20             1.00  ...                  NaN   \n",
       "1283        1.20               20             1.00  ...                  1.0   \n",
       "1284        1.08               20             1.00  ...                  1.0   \n",
       "1285        1.24               20             1.00  ...                  1.0   \n",
       "\n",
       "      Concreteness_Source  Zipf_EsPal        POS  Emotionality      Family  \\\n",
       "0                     NEW        4.36  ADJECTIVE          3.07   abandonar   \n",
       "1                     NEW        2.98       VERB          3.11   abandonar   \n",
       "2                     NEW        4.31       NOUN          2.70   abandonar   \n",
       "3                     NEW        3.59  ADJECTIVE          2.23      abatir   \n",
       "4                     NEW        3.28       NOUN          2.20      abatir   \n",
       "...                   ...         ...        ...           ...         ...   \n",
       "1281                  NEW        4.50  ADJECTIVE          1.35       vital   \n",
       "1282           Duchon2013        3.74       NOUN          3.20       vital   \n",
       "1283                  NEW        2.03       VERB          1.80       vital   \n",
       "1284                  NEW        3.49       NOUN          2.70  vulnerable   \n",
       "1285                  NEW        3.70  ADJECTIVE          2.20  vulnerable   \n",
       "\n",
       "      Very_related_emotions  Pure_word  Dominant_emotion  \\\n",
       "0                       2.0        0.0           sadness   \n",
       "1                       2.0        0.0           sadness   \n",
       "2                       3.0        0.0           sadness   \n",
       "3                       1.0        1.0           sadness   \n",
       "4                       1.0        1.0           sadness   \n",
       "...                     ...        ...               ...   \n",
       "1281                    0.0        0.0         happiness   \n",
       "1282                    1.0        1.0         happiness   \n",
       "1283                    1.0        1.0         happiness   \n",
       "1284                    1.0        1.0           sadness   \n",
       "1285                    1.0        1.0           sadness   \n",
       "\n",
       "      Emotional_exclusivity  \n",
       "0                     0.396  \n",
       "1                     0.326  \n",
       "2                     0.350  \n",
       "3                     0.379  \n",
       "4                     0.340  \n",
       "...                     ...  \n",
       "1281                  0.706  \n",
       "1282                  0.901  \n",
       "1283                  0.741  \n",
       "1284                  0.288  \n",
       "1285                  0.274  \n",
       "\n",
       "[1286 rows x 59 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabrasProto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selección de altamente prototípias de EmoPro\n",
    "altaProto = palabrasProto[palabrasProto['Prototypicality_Mean']>= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Few_Raters</th>\n",
       "      <th>Prototypicality_Mean</th>\n",
       "      <th>Prototypicality_SD</th>\n",
       "      <th>Prototypicality_NRaters</th>\n",
       "      <th>Prototypicality_%Raters</th>\n",
       "      <th>Valence_Mean</th>\n",
       "      <th>Valence_SD</th>\n",
       "      <th>Valence_NRaters</th>\n",
       "      <th>Valence_%Raters</th>\n",
       "      <th>...</th>\n",
       "      <th>Concreteness_%Raters</th>\n",
       "      <th>Concreteness_Source</th>\n",
       "      <th>Zipf_EsPal</th>\n",
       "      <th>POS</th>\n",
       "      <th>Emotionality</th>\n",
       "      <th>Family</th>\n",
       "      <th>Very_related_emotions</th>\n",
       "      <th>Pure_word</th>\n",
       "      <th>Dominant_emotion</th>\n",
       "      <th>Emotional_exclusivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abatido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.75</td>\n",
       "      <td>26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.59</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.23</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abatimiento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.20</td>\n",
       "      <td>abatir</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abochornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.37</td>\n",
       "      <td>21</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NEW</td>\n",
       "      <td>2.24</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.55</td>\n",
       "      <td>bochorno</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abochornarse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>21</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.18</td>\n",
       "      <td>26</td>\n",
       "      <td>0.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NEW</td>\n",
       "      <td>0.51</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1.56</td>\n",
       "      <td>bochorno</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>abrumado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.24</td>\n",
       "      <td>1.48</td>\n",
       "      <td>22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.48</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.24</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>1.10</td>\n",
       "      <td>abrumar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>vergonzoso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.31</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEW</td>\n",
       "      <td>3.57</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.45</td>\n",
       "      <td>vergüenza</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>vergüenza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.97</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.18</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEW</td>\n",
       "      <td>4.37</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>vergüenza</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>victorioso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.13</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hinojosa2016_BRM</td>\n",
       "      <td>3.55</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>2.05</td>\n",
       "      <td>victoria</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>violentarse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>21</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.11</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NEW</td>\n",
       "      <td>1.63</td>\n",
       "      <td>VERB</td>\n",
       "      <td>3.20</td>\n",
       "      <td>violento</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>violento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.25</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.17</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guasch2016</td>\n",
       "      <td>4.08</td>\n",
       "      <td>ADJECTIVE</td>\n",
       "      <td>3.10</td>\n",
       "      <td>violento</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word Few_Raters  Prototypicality_Mean  Prototypicality_SD  \\\n",
       "3          abatido        NaN                  3.37                1.42   \n",
       "4      abatimiento        NaN                  3.00                1.37   \n",
       "6      abochornado        NaN                  3.90                1.37   \n",
       "7     abochornarse        NaN                  3.84                1.54   \n",
       "13        abrumado        NaN                  3.24                1.48   \n",
       "...            ...        ...                   ...                 ...   \n",
       "1270    vergonzoso        NaN                  3.60                1.31   \n",
       "1271     vergüenza        NaN                  4.38                0.97   \n",
       "1273    victorioso        NaN                  3.70                1.13   \n",
       "1279   violentarse        NaN                  3.05                1.47   \n",
       "1280      violento        NaN                  3.43                1.25   \n",
       "\n",
       "      Prototypicality_NRaters  Prototypicality_%Raters  Valence_Mean  \\\n",
       "3                          20                     0.95          2.77   \n",
       "4                          20                     0.90          2.80   \n",
       "6                          21                     0.95          2.45   \n",
       "7                          21                     0.90          3.44   \n",
       "13                         22                     0.95          3.90   \n",
       "...                       ...                      ...           ...   \n",
       "1270                       20                     1.00          2.55   \n",
       "1271                       21                     1.00          3.85   \n",
       "1273                       20                     1.00          7.05   \n",
       "1279                       21                     0.90          1.80   \n",
       "1280                       21                     1.00          1.90   \n",
       "\n",
       "      Valence_SD  Valence_NRaters  Valence_%Raters  ... Concreteness_%Raters  \\\n",
       "3           1.75               26             1.00  ...                 1.00   \n",
       "4           1.32               20             1.00  ...                 1.00   \n",
       "6           1.28               20             1.00  ...                 0.95   \n",
       "7           2.18               26             0.96  ...                 0.95   \n",
       "13          1.48               20             1.00  ...                 1.00   \n",
       "...          ...              ...              ...  ...                  ...   \n",
       "1270        1.28               20             1.00  ...                 1.00   \n",
       "1271        1.18               20             1.00  ...                 1.00   \n",
       "1273        1.47               20             1.00  ...                  NaN   \n",
       "1279        1.11               20             1.00  ...                 0.95   \n",
       "1280        1.17               20             1.00  ...                  NaN   \n",
       "\n",
       "      Concreteness_Source  Zipf_EsPal        POS  Emotionality     Family  \\\n",
       "3                     NEW        3.59  ADJECTIVE          2.23     abatir   \n",
       "4                     NEW        3.28       NOUN          2.20     abatir   \n",
       "6                     NEW        2.24  ADJECTIVE          2.55   bochorno   \n",
       "7                     NEW        0.51       VERB          1.56   bochorno   \n",
       "13                    NEW        3.24  ADJECTIVE          1.10    abrumar   \n",
       "...                   ...         ...        ...           ...        ...   \n",
       "1270                  NEW        3.57  ADJECTIVE          2.45  vergüenza   \n",
       "1271                  NEW        4.37       NOUN          1.15  vergüenza   \n",
       "1273     Hinojosa2016_BRM        3.55  ADJECTIVE          2.05   victoria   \n",
       "1279                  NEW        1.63       VERB          3.20   violento   \n",
       "1280           Guasch2016        4.08  ADJECTIVE          3.10   violento   \n",
       "\n",
       "      Very_related_emotions  Pure_word  Dominant_emotion  \\\n",
       "3                       1.0        1.0           sadness   \n",
       "4                       1.0        1.0           sadness   \n",
       "6                       2.0        0.0             anger   \n",
       "7                       0.0        0.0             anger   \n",
       "13                      0.0        0.0           sadness   \n",
       "...                     ...        ...               ...   \n",
       "1270                    0.0        0.0           sadness   \n",
       "1271                    1.0        1.0              fear   \n",
       "1273                    1.0        1.0         happiness   \n",
       "1279                    3.0        0.0             anger   \n",
       "1280                    4.0        0.0             anger   \n",
       "\n",
       "      Emotional_exclusivity  \n",
       "3                     0.379  \n",
       "4                     0.340  \n",
       "6                     0.264  \n",
       "7                     0.248  \n",
       "13                    0.269  \n",
       "...                     ...  \n",
       "1270                  0.294  \n",
       "1271                  0.260  \n",
       "1273                  0.813  \n",
       "1279                  0.295  \n",
       "1280                  0.293  \n",
       "\n",
       "[549 rows x 59 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altaProto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Igualar número con resultados de fractalidad e0\n",
    "fractPositivas = fractPositivas[:549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caca</td>\n",
       "      <td>23</td>\n",
       "      <td>85221.880907</td>\n",
       "      <td>116049.007469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok</td>\n",
       "      <td>12</td>\n",
       "      <td>78526.466331</td>\n",
       "      <td>84744.289783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capsulas</td>\n",
       "      <td>11</td>\n",
       "      <td>78681.018360</td>\n",
       "      <td>81937.836981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>observaciones</td>\n",
       "      <td>19</td>\n",
       "      <td>63249.000502</td>\n",
       "      <td>80879.887149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calzoncillo</td>\n",
       "      <td>11</td>\n",
       "      <td>77619.128001</td>\n",
       "      <td>80831.992129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>muscular</td>\n",
       "      <td>5</td>\n",
       "      <td>36518.004889</td>\n",
       "      <td>25524.990036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>pasos</td>\n",
       "      <td>5</td>\n",
       "      <td>36510.492729</td>\n",
       "      <td>25519.739261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>prescripción</td>\n",
       "      <td>6</td>\n",
       "      <td>32758.927861</td>\n",
       "      <td>25491.400677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>quiste</td>\n",
       "      <td>3</td>\n",
       "      <td>53404.728254</td>\n",
       "      <td>25480.530953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>culo</td>\n",
       "      <td>3</td>\n",
       "      <td>53400.626041</td>\n",
       "      <td>25478.573700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0             caca         23          85221.880907     116049.007469\n",
       "1               ok         12          78526.466331      84744.289783\n",
       "2         capsulas         11          78681.018360      81937.836981\n",
       "3    observaciones         19          63249.000502      80879.887149\n",
       "4      calzoncillo         11          77619.128001      80831.992129\n",
       "..             ...        ...                   ...               ...\n",
       "544       muscular          5          36518.004889      25524.990036\n",
       "545          pasos          5          36510.492729      25519.739261\n",
       "546   prescripción          6          32758.927861      25491.400677\n",
       "547         quiste          3          53404.728254      25480.530953\n",
       "548           culo          3          53400.626041      25478.573700\n",
       "\n",
       "[549 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractPositivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Igualar número con resultados de fractalidad e5\n",
    "fractNegativas = fractNegativas[:549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ángel</td>\n",
       "      <td>10</td>\n",
       "      <td>18176.971038</td>\n",
       "      <td>18176.971038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tío</td>\n",
       "      <td>18</td>\n",
       "      <td>12706.403384</td>\n",
       "      <td>15949.998807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siento</td>\n",
       "      <td>118</td>\n",
       "      <td>6956.870176</td>\n",
       "      <td>14413.814144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vómitos</td>\n",
       "      <td>7</td>\n",
       "      <td>16922.938652</td>\n",
       "      <td>14301.542286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vida</td>\n",
       "      <td>82</td>\n",
       "      <td>7257.344789</td>\n",
       "      <td>13889.206989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>quiera</td>\n",
       "      <td>3</td>\n",
       "      <td>7920.344064</td>\n",
       "      <td>3778.964498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>cabreado</td>\n",
       "      <td>3</td>\n",
       "      <td>7907.603557</td>\n",
       "      <td>3772.885731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>padres</td>\n",
       "      <td>5</td>\n",
       "      <td>5378.794742</td>\n",
       "      <td>3759.616184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>quede</td>\n",
       "      <td>3</td>\n",
       "      <td>7852.723224</td>\n",
       "      <td>3746.701158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>motiva</td>\n",
       "      <td>4</td>\n",
       "      <td>6218.445007</td>\n",
       "      <td>3743.876947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0       ángel         10          18176.971038      18176.971038\n",
       "1         tío         18          12706.403384      15949.998807\n",
       "2      siento        118           6956.870176      14413.814144\n",
       "3     vómitos          7          16922.938652      14301.542286\n",
       "4        vida         82           7257.344789      13889.206989\n",
       "..        ...        ...                   ...               ...\n",
       "544    quiera          3           7920.344064       3778.964498\n",
       "545  cabreado          3           7907.603557       3772.885731\n",
       "546    padres          5           5378.794742       3759.616184\n",
       "547     quede          3           7852.723224       3746.701158\n",
       "548    motiva          4           6218.445007       3743.876947\n",
       "\n",
       "[549 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractNegativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpos = trpos[:549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard de fractalidad positivas:  0.025139664804469275\n",
      "Jaccard de TextRank positivas:  0.00725689404934688\n",
      "Jaccard de fractalidad negativas:  0.028011204481792718\n",
      "Jaccard de TextRank negativas:  0.00725689404934688\n"
     ]
    }
   ],
   "source": [
    "jaccFractalidadPos = jaccard(set(palabrasProto['Word']), set(fractPositivas['word']))\n",
    "jaccTRPos = jaccard(set(palabrasProto['Word']), set(trpos))\n",
    "jaccFractalidadNeg = jaccard(set(palabrasProto['Word']), set(fractNegativas['word']))\n",
    "jaccTRNeg = jaccard(set(palabrasProto['Word']), set(trneg))\n",
    "# _, conteosTR = jaccard(texto, dftr)\n",
    "# _, conteosFract= jaccard(texto, list(df1['word']))\n",
    "# _, conteosProto= jaccard(texto, list(palabrasProto['Palabra']))\n",
    "\n",
    "print('Jaccard de fractalidad positivas: ', jaccFractalidadPos)\n",
    "\n",
    "print('Jaccard de TextRank positivas: ', jaccTRPos)\n",
    "\n",
    "\n",
    "print('Jaccard de fractalidad negativas: ', jaccFractalidadNeg)\n",
    "\n",
    "print('Jaccard de TextRank negativas: ', jaccTRNeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinTRPos, conteoTRPos= coinsidencia(list(palabrasProto['Word']), trpos)\n",
    "coinTRNeg, conteoTRNeg = coinsidencia(list(palabrasProto['Word']), trneg)\n",
    "coinFracPos, conteoFracPos = coinsidencia(list(palabrasProto['Word']), list(fractPositivas['word']))\n",
    "coinFracNeg, conteoFracNeg = coinsidencia(list(palabrasProto['Word']), list(fractNegativas['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo TR positivas: 10\n",
      "Conteo TR negativas: 10\n",
      "Conteo Fractalidad positivas: 45\n",
      "Conteo Fractalidad negativas: 50\n"
     ]
    }
   ],
   "source": [
    "print('Conteo TR positivas:', conteoTRPos)\n",
    "print('Conteo TR negativas:', conteoTRNeg)\n",
    "print('Conteo Fractalidad positivas:', conteoFracPos)\n",
    "print('Conteo Fractalidad negativas:', conteoFracNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRak, a pesar de que se le solicitaron 1286 y 549 palabras clave, sólo regresó 102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
