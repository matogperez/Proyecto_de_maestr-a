{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline de algoritmos de xtracción de palabras clave"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos básicos: TextRank y Fractalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Notebook es parte de una serie de ejercicios para comprobar la medida de jaccard entre el corpus original y el EmoPro con los algoritmos clásicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendientes de este documento:\n",
    "- Cambiar las direcciones para que obtenga las palabras del EmoPro\n",
    "- Cambiar la función de Jaccard\n",
    "- Hacer la comprobación para las palabras altamente prototípicas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importes para librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from math import sqrt\n",
    "import string\n",
    "# import csv\n",
    "import operator\n",
    "# import random\n",
    "import pandas as pd\n",
    "#librerias necesarias para text rank\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "#import nlp\n",
    "\n",
    "#Listado de STOPWORDS dependiendo del lenguaje\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la clase TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 100 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "\n",
    "\n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        keysw={}\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            keysw[key] =value\n",
    "            if i > number:\n",
    "                break\n",
    "        return keysw\n",
    "\n",
    "\n",
    "    def analyze(self, text,\n",
    "                candidate_pos=['NOUN', 'PROPN'],\n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la función Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solamente se calcula el grado de fractalidad de las palabras que tengan mas de uno de frecuencia\n",
    "def fractalidad(palabras,vocabulario,frec,dist):\n",
    "    N=len(palabras)                                     #El número de tokens de todo el texto\n",
    "    gf={}\n",
    "    cajas_index=set()\n",
    "    voc=[]                                             #la variable voc contendra cada sintagma con frecuencia mayor que 1, por que las otras palabras tendrán 0 de grado de fractaldiad\n",
    "    for p in vocabulario:                              #Esto se puede hacer fuera del algoritmo, pero se incluye para evitar ese calculo innecesario\n",
    "        if(p not in voc):\n",
    "            if(frec[p]>1):\n",
    "                if(p not in STOP_WORDS):\n",
    "                    if(len(p)>1):\n",
    "                        voc.append(p)\n",
    "    # print(\"Text size: \",N)\n",
    "    # print(\"Vocabulary: \",len(voc))\n",
    "    for p in voc:\n",
    "        rcajas=dist[p]\n",
    "        M=frec[p]\n",
    "        dfw=0.0\n",
    "        nsh=0.0\n",
    "        for s in range(1,N+1):\n",
    "            noc=0\n",
    "            for e in rcajas:\n",
    "                cajas_index.add(ceil(int(e)/s))\n",
    "            noc=len(cajas_index)\n",
    "            cajas_index.clear()\n",
    "            ns=N/s\n",
    "            if(M<=ns):\n",
    "                nsh=M\n",
    "            else:\n",
    "                nsh=M/(1+(M-1)/(N-1)*(s-1))\n",
    "            dfw=dfw+fabs(log(nsh/noc))\n",
    "        gf[p]=dfw\n",
    "    return gf    #regresamos un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribucion(palabras,vocabulario):\n",
    "    N=len(palabras)\n",
    "    ncajas=[]\n",
    "    cajas={}\n",
    "    frecuencias={}\n",
    "    for p in vocabulario:\n",
    "        ncajas.clear()\n",
    "        i=0\n",
    "        M=palabras.count(p)\n",
    "        while(i<N):\n",
    "            if(p == palabras[i]):\n",
    "                ncajas.append(i+1)\n",
    "            i=i+1\n",
    "        frecuencias[p]=M\n",
    "        cajas[p]=ncajas[:]\n",
    "    return frecuencias,cajas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay que trabajar en la lectura de los datos de forma que sólo se quede la columna del texto. Es probable que haya que reescribir todo esto para descargarlo a través de pandas\n",
    "#Igual es necesario conservar la transformación para quitar los signos de punbtuación y mantener el texto en minúsculas\n",
    "#Lectura de archivo para generación de vocabulario\n",
    "def cargar_datos(filename):\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        texto = pd.read_csv(f)\n",
    "        texto = texto['Texto']\n",
    "        for row in texto:\n",
    "            #Pasar a minusculas\n",
    "            row = ' '.join(row)\n",
    "            row = row.lower()\n",
    "    #Eliminar puntuación\n",
    "    texto =' '.join(texto)\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    texto = texto.translate(str.maketrans('', '', '¿¡—“”0123456789’'))\n",
    "    palabras = texto.split()\n",
    "    textop=\"\"\n",
    "    #rearmamos el texto debido a que existen carácteres especiales\n",
    "    for w in palabras:\n",
    "        textop=textop+w+' '\n",
    "    return textop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_documento(filename='NULL',texto=''):\n",
    "    if filename != 'NULL':\n",
    "        texto=cargar_datos(filename)\n",
    "    #obtenemos el vocabulario\n",
    "    tokens=texto.split()\n",
    "    vocabulario=[]\n",
    "    for t in tokens:\n",
    "        if(t not in vocabulario):\n",
    "            vocabulario.append(t)\n",
    "    #variables de procesamiento\n",
    "    dist={}\n",
    "    frec={}\n",
    "    frec,dist=distribucion(tokens,vocabulario)\n",
    "    return frec,dist,tokens,vocabulario,texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de documento para el corpus de suicidio\n",
    "frec,dist,tokens,vocabulario,texto = lee_documento('/home/matias/Documentos/MGP/Proyecto_tesis/Dataset/SMS_DATA_ORIGINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuencias: 12564\n",
      "Distribuciones:  12564\n",
      "Tokens:  246686\n",
      "Vocabulario:  12564\n",
      "Texto:  1302315\n"
     ]
    }
   ],
   "source": [
    "print('Frecuencias:', len(frec))\n",
    "print(\"Distribuciones: \", len(dist))\n",
    "print(\"Tokens: \", len(tokens))\n",
    "print('Vocabulario: ', len(vocabulario))\n",
    "print('Texto: ', len(texto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caca</td>\n",
       "      <td>23</td>\n",
       "      <td>211335.717556</td>\n",
       "      <td>287781.729340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ansium</td>\n",
       "      <td>29</td>\n",
       "      <td>186659.515902</td>\n",
       "      <td>272970.502344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tendido</td>\n",
       "      <td>23</td>\n",
       "      <td>181585.856566</td>\n",
       "      <td>247270.515513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recogido</td>\n",
       "      <td>182</td>\n",
       "      <td>109372.307097</td>\n",
       "      <td>247189.221909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naltrexona</td>\n",
       "      <td>16</td>\n",
       "      <td>197871.809551</td>\n",
       "      <td>238261.399885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>caramelos</td>\n",
       "      <td>2</td>\n",
       "      <td>85040.677669</td>\n",
       "      <td>25599.794830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>instrumento</td>\n",
       "      <td>2</td>\n",
       "      <td>84619.035790</td>\n",
       "      <td>25472.867977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>caray</td>\n",
       "      <td>2</td>\n",
       "      <td>84598.145385</td>\n",
       "      <td>25466.579339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>suelen</td>\n",
       "      <td>2</td>\n",
       "      <td>84552.569731</td>\n",
       "      <td>25452.859699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>Lidl</td>\n",
       "      <td>2</td>\n",
       "      <td>84476.844629</td>\n",
       "      <td>25430.064172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1286 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0            caca         23         211335.717556     287781.729340\n",
       "1          Ansium         29         186659.515902     272970.502344\n",
       "2         tendido         23         181585.856566     247270.515513\n",
       "3        recogido        182         109372.307097     247189.221909\n",
       "4      Naltrexona         16         197871.809551     238261.399885\n",
       "...           ...        ...                   ...               ...\n",
       "1281    caramelos          2          85040.677669      25599.794830\n",
       "1282  instrumento          2          84619.035790      25472.867977\n",
       "1283        caray          2          84598.145385      25466.579339\n",
       "1284       suelen          2          84552.569731      25452.859699\n",
       "1285         Lidl          2          84476.844629      25430.064172\n",
       "\n",
       "[1286 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ejecución de algoritmo Grado de Fractalidad\n",
    "def grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw=False,regresa_df=True,top_n=np.inf,escribe_arch=False):\n",
    "    frac_x=fractalidad(tokens,vocabulario,frec,dist)\n",
    "    sorted_x = sorted(frac_x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # print('Time GF: '+str(elapsed_time))\n",
    "\n",
    "    #Imprimir y guardar resultados de GF\n",
    "    if regresa_df:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            df = [dato[0] for dato in df]\n",
    "            by_MC=pd.DataFrame(df, columns=['word'])\n",
    "        else:\n",
    "            by_MC=pd.DataFrame(df, columns=['word','frecuency','Degree_of_fractality','Combined_measure'])\n",
    "        if escribe_arch:\n",
    "            by_MC.to_csv('GF.csv')\n",
    "    else:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            by_MC = [dato[0] for dato in df]\n",
    "        else:\n",
    "            by_MC = df\n",
    "        if escribe_arch:\n",
    "            print('\\nNo se tiene implementada la escritura de archivo cuando regresa_df==False\\n')\n",
    "    return by_MC\n",
    "\n",
    "def use_gf(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tokens=texto.split()\n",
    "    vocabulario=[]\n",
    "    for t in tokens:\n",
    "        if(t not in vocabulario):\n",
    "            vocabulario.append(t)\n",
    "    #variables de procesamiento\n",
    "    dist={}\n",
    "    frec={}\n",
    "    frec,dist=distribucion(tokens,vocabulario)\n",
    "    df = grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw,regresa_df,top_n,escribe_arch)\n",
    "    return df\n",
    "\n",
    "df1 = use_gf(texto,regresa_kw=True,regresa_df=True,top_n=1286)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guarda los resultados del grado de fractalidad en un CSV. Siempre hay que cambiar la versión y comentar para no sobreescribir\n",
    "df1.to_csv('/home/matias/Documentos/MGP/Proyecto_tesis/output/fractalidad_EmoPro_resultados_Corpus_Completo.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio del máximo de tamaño de texto para que quepa el corpus (1.3 millones)\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosas',\n",
       " 'casa',\n",
       " 'trabajo',\n",
       " 'vida',\n",
       " 'madre',\n",
       " 'tiempo',\n",
       " 'semana',\n",
       " 'ansiedad',\n",
       " 'dolor',\n",
       " 'ganas',\n",
       " 'cabeza',\n",
       " 'noche',\n",
       " 'problemas',\n",
       " 'horas',\n",
       " 'mañana',\n",
       " 'medicación',\n",
       " 'familia',\n",
       " 'miedo',\n",
       " 'gracias',\n",
       " 'situación',\n",
       " 'sueño',\n",
       " 'años',\n",
       " 'sensación',\n",
       " 'gente',\n",
       " 'momento',\n",
       " 'persona',\n",
       " 'ánimo',\n",
       " 'problema',\n",
       " 'cosa',\n",
       " 'padre',\n",
       " 'hora',\n",
       " 'comida',\n",
       " 'tratamiento',\n",
       " 'marido',\n",
       " 'pareja',\n",
       " 'perro',\n",
       " 'hija',\n",
       " 'momentos',\n",
       " 'médico',\n",
       " 'mujer',\n",
       " 'tensión',\n",
       " 'cama',\n",
       " 'doctora',\n",
       " 'cuerpo',\n",
       " 'pensamientos',\n",
       " 'hijos',\n",
       " 'dolores',\n",
       " 'intento',\n",
       " 'general',\n",
       " 'mundo',\n",
       " 'cambio',\n",
       " 'consulta',\n",
       " 'pastillas',\n",
       " 'nervios',\n",
       " 'hermana',\n",
       " 'hijo',\n",
       " 'mes',\n",
       " 'enfermedad',\n",
       " 'mierda',\n",
       " 'tema',\n",
       " 'noches',\n",
       " 'personas',\n",
       " 'hospital',\n",
       " 'meses',\n",
       " 'vacaciones',\n",
       " 'cambios',\n",
       " 'sigo',\n",
       " 'pena',\n",
       " 'cita',\n",
       " 'año',\n",
       " 'salud',\n",
       " 'falta',\n",
       " 'rato',\n",
       " 'sábado',\n",
       " 'dinero',\n",
       " 'amigos',\n",
       " 'amiga',\n",
       " 'calle',\n",
       " 'psiquiatra',\n",
       " 'forma',\n",
       " 'cuestionario',\n",
       " 'daño',\n",
       " 'fuerzas',\n",
       " 'Madrid',\n",
       " 'cansancio',\n",
       " 'semanas',\n",
       " 'mente',\n",
       " 'tristeza',\n",
       " 'coche',\n",
       " 'baja',\n",
       " 'discusión',\n",
       " 'resto',\n",
       " 'culpa',\n",
       " 'relación',\n",
       " 'viernes',\n",
       " 'hola',\n",
       " 'estrés',\n",
       " 'caso',\n",
       " 'domingo',\n",
       " 'miligramo',\n",
       " 'actividades',\n",
       " 'hermano']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ejecución de algoritmo de TextRank\n",
    "# start_time = time()\n",
    "def use_TextRank(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(texto, candidate_pos = ['NOUN','PROPN'], window_size=4, lower=False)\n",
    "    kwTR=tr4w.get_keywords(100)\n",
    "\n",
    "    #Guardar resultados de TextRank\n",
    "    if regresa_df:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())[:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "    else:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())[:top_n]\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())\n",
    "        # elapsed_time = time() - start_time\n",
    "        # print('Time TextRank: '+str(elapsed_time))\n",
    "        if escribe_arch:\n",
    "            dftr.to_csv('TextRank.csv')\n",
    "    return dftr\n",
    "\n",
    "dftr = use_TextRank(texto,top_n=1286)\n",
    "dftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guarda los resultados del TextRank en un txt. Siempre hay que cambiar la versión y comentar para no sobreescribir\n",
    "with open('/home/matias/Documentos/MGP/Proyecto_tesis/output/TextRank_EmoPro_resultados_Corpus_Completo.txt','w') as output:\n",
    "    output.write(str(dftr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de los resultados de fractalidad y TextRank con el conjunto de Palabras Prototípicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/matias/Documentos/MGP/Proyecto_tesis/Palabras_proto/EmoPro-Dataset.csv', encoding=\"utf-8-sig\") as f:\n",
    "        palabrasProto = pd.read_csv(f)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para crear la función de medida de Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1: set, s2: set):\n",
    "    return len(s1 & s2) / len(s1 | s2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coinsidencia(conjuntoA, conjuntoB):\n",
    "    matches = []\n",
    "    for palabra in conjuntoA:\n",
    "        if palabra in conjuntoB:\n",
    "            matches.append(palabra)\n",
    "    conteo = len(matches)\n",
    "    return matches, conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medida Jaccard Grado de Fractalidad: 0.027156549520766772\n",
      "Medida Jaccard TextRank: 0.007988380537400145\n"
     ]
    }
   ],
   "source": [
    "jaccFract = jaccard(set(palabrasProto['Word']), set(df1['word']))\n",
    "jaccTR = jaccard(set(palabrasProto['Word']), set(dftr))\n",
    "\n",
    "print(\"Medida Jaccard Grado de Fractalidad:\", jaccFract)\n",
    "print(\"Medida Jaccard TextRank:\", jaccTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de coinsidencias Fractalidad: 68\n",
      "Conteo de coinsidencias TextRank: 11\n"
     ]
    }
   ],
   "source": [
    "coinFract, conteoFrac = coinsidencia(list(palabrasProto['Word']), list(df1['word']))\n",
    "coinTR, conteoTR = coinsidencia(list(palabrasProto['Word']),list(dftr))\n",
    "\n",
    "print('Conteo de coinsidencias Fractalidad:', conteoFrac)\n",
    "print('Conteo de coinsidencias TextRank:', conteoTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdfrt\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfrt' is not defined"
     ]
    }
   ],
   "source": [
    "len(dfrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendientes\n",
    "Hacer la comparación con altamente prototípicas:\n",
    "- Recuperar los datos del de fractalidad por medio del archivo guardado\n",
    "- Volver a correr y comprobar la cantidad de palabras que regresa el TextRank\n",
    "- Hacer las comparaciones e imprimir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altasProtp = palabrasProto[palabrasProto['Prototypicality_Mean']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractal = pd.read_csv('/home/matias/Documentos/MGP/Proyecto_tesis/output/fractalidad_EmoPro_resultados_Corpus_Completo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractal = dfFractal[:549]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
